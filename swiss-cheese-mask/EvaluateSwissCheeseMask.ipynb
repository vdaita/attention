{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca481ef7-bf6a-426e-a26a-06aa7c2d0de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.generation import _crop_past_key_values\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8628d1-cb01-43a3-83df-abb3e767593e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def key_values_indices(model, past_key_values, indices: torch.tensor):\n",
    "    \"\"\"Crops the past key values up to a certain maximum length.\"\"\"\n",
    "    new_past = []\n",
    "    if model.config.is_encoder_decoder:\n",
    "        for idx in range(len(past_key_values)):\n",
    "            new_past.append(\n",
    "                (\n",
    "                    past_key_values[idx][0][:, :, indices, :],\n",
    "                    past_key_values[idx][1][:, :, indices, :],\n",
    "                    past_key_values[idx][2],\n",
    "                    past_key_values[idx][3],\n",
    "                )\n",
    "            )\n",
    "        past_key_values = tuple(new_past)\n",
    "    # bloom is special\n",
    "    elif \"bloom\" in model.__class__.__name__.lower() or (\n",
    "        model.config.architectures is not None and \"bloom\" in model.config.architectures[0].lower()\n",
    "    ):\n",
    "        for idx in range(len(past_key_values)):\n",
    "            new_past.append(\n",
    "                (\n",
    "                    past_key_values[idx][0][:, :, indices],\n",
    "                    past_key_values[idx][1][:, indices, :],\n",
    "                )\n",
    "            )\n",
    "        past_key_values = tuple(new_past)\n",
    "    # gptbigcode is too\n",
    "    elif \"gptbigcode\" in model.__class__.__name__.lower() or (\n",
    "        model.config.architectures is not None and \"gptbigcode\" in model.config.architectures[0].lower()\n",
    "    ):\n",
    "        if model.config.multi_query:\n",
    "            for idx in range(len(past_key_values)):\n",
    "                past_key_values[idx] = past_key_values[idx][:, indices, :]\n",
    "        else:\n",
    "            for idx in range(len(past_key_values)):\n",
    "                past_key_values[idx] = past_key_values[idx][:, :, indices, :]\n",
    "    elif isinstance(past_key_values, DynamicCache):\n",
    "        past_key_values.crop(max_length)\n",
    "    elif past_key_values is not None:\n",
    "        for idx in range(len(past_key_values)):\n",
    "            new_past.append(\n",
    "                (\n",
    "                    past_key_values[idx][0][:, :, indices, :],\n",
    "                    past_key_values[idx][1][:, :, indices, :],\n",
    "                )\n",
    "            )\n",
    "        past_key_values = tuple(new_past)\n",
    "    return past_key_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09189425-a47a-4f80-93e0-32a3150c858c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def efficient_generate(model, tokenizer, input_ids, max_tokens=200, sink_length=8, end_length=64, window_stride=8):\n",
    "    # prefill stage: load all of the inputs (since prefilling is fast, this shouldn't take too much time)\n",
    "    current_tokens = input_ids\n",
    "    outputs = model(\n",
    "        input_ids\n",
    "    )\n",
    "    \n",
    "    # TODO: attach the generated token to the end of the current tokens\n",
    "    past_key_values = outputs.past_key_values\n",
    "\n",
    "    # build the mask\n",
    "    mask = torch.zeros(current_tokens.shape[-1])\n",
    "    mask[:sink_length] = 1\n",
    "    mask[-end_length:] = 1\n",
    "    mask[::window_stride] = 1\n",
    "\n",
    "    input_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "    punctuation_chars = set(\".!?;:\") + set(\"\\n\")\n",
    "    punctuation_token_idx = [input_id for input_id, token in zip(input_ids, tokens) if any(char in token in char in punctuation_chars)]\n",
    "    punctuation_token_indices = [idx for idx, (input_id, token) in enumerate(zip(input_ids, tokens)) if any(char in token in char in punctuation_chars)]\n",
    "    mask[punctuation_token_indices] = 1\n",
    "    \n",
    "    # save the relevant ones\n",
    "    context_tokens = current_tokens[mask]\n",
    "    past_key_values = key_values_indices(model, past_key_values, torch.nonzero(mask == 1).squeeze())\n",
    "\n",
    "    # generate subsequent tokens\n",
    "    while True:\n",
    "        # check if the length is long enough, and if so, confirm whether or not the last token of the sliding window left will be staying on\n",
    "            # if so, keep the KV cache the same\n",
    "            # otherwise, prune the KV cache\n",
    "        if len(context_tokens) >= end_length:\n",
    "            if not(\n",
    "                (len(context_tokens) - end_length - 1) % window_stride == 0\n",
    "                or\n",
    "                (context_tokens[-(end_length + 1)] in punctuation_token_idx)\n",
    "                or\n",
    "                (len(context_tokens) - end_length - 1) < sink_length\n",
    "            ):\n",
    "                del_idx = len(context_tokens) - end_length - 1\n",
    "                past_key_values = key_values_indices(model, past_key_values, torch.Tensor([j for j in range(len(context_tokens)) if j != del_idx]))\n",
    "                context_tokens = torch.cat((context_tokens[:-del_idx], context_tokens[-(del_idx - 1):]))\n",
    "        outputs = model(\n",
    "            context_tokens,\n",
    "            past_key_values=past_key_values\n",
    "        )\n",
    "        \n",
    "        \n",
    "        # calculate the next tokens\n",
    "        \n",
    "        if current_tokens.shape[-1] >= max_tokens:\n",
    "            break\n",
    "        if current_tokens[-1] == tokenizer.eos_token:\n",
    "            break\n",
    "\n",
    "    return tokenizer.decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673179f7-d66d-40bd-a07c-573c71c043b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_attention_mask():\n",
    "    ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
