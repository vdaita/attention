I'm using this project to help me learn, at a bit of a lower level, how different forms of attention work and how custom modifications can be made to standard tooling (like HF transformers). 

Right now, I'm testing out a custom attention mask and seeing if adding a special embedding for tokens that are actually attended to lead to any changes or impact on loss. Testing this out using TinyStories.